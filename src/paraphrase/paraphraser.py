from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Union
from transformers import T5ForConditionalGeneration, PreTrainedTokenizer, PreTrainedModel, T5Tokenizer, T5ForConditionalGeneration
import torch

# Define a data class to hold model configuration
@dataclass
class ModelConfig:
    max_length: int
    do_sample: bool
    top_k: int
    top_p: float
    repetition_penalty: float
    early_stopping: bool
    num_return_sequences: int


class Paraphraser(ABC):
    @abstractmethod
    def paraphrase(self, sentence: str) -> List[str]:
        pass

class T5Paraphraser(Paraphraser):
    def __init__(self, 
                 model_t5: Union[T5ForConditionalGeneration, PreTrainedModel], 
                 tokenizer_t5: PreTrainedTokenizer, 
                 config: ModelConfig):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_t5 = model_t5.to(self.device)
        self.tokenizer_t5 = tokenizer_t5
        self.config = config

    """
    Generates paraphrases for a given sentence using a T5 model.
    
    Parameters:
    org_sentence (str): Original sentence to paraphrase.
    model_t5 (Union[T5ForConditionalGeneration, PreTrainedModel]): The T5 model to use for paraphrasing.
    tokenizer_t5 (PreTrainedTokenizer): The tokenizer to use.
    config (ModelConfig): The configuration for the model.
    
    Returns:
    List[str]: List of paraphrases generated by the model.
    """
    
    def set_model_config(self, config: ModelConfig):
        self.config = config

    def set_model(self, model_t5: Union[T5ForConditionalGeneration, PreTrainedModel]):
        self.model_t5 = model_t5.to(self.device)

    def set_tokenizer(self, tokenizer_t5: PreTrainedTokenizer):
        self.tokenizer_t5 = tokenizer_t5

    def paraphrase(self, sentence: str) -> List[str]:
        return self._get_t5_gen_sentences(sentence)

    def _get_t5_gen_sentences(self, org_sentence: str) -> List[str]:
        text =  "paraphrase: " + org_sentence 
        # encoding = self.tokenizer_t5.encode_plus(text, pad_to_max_length=True, return_tensors="pt")
        encoding = self.tokenizer_t5.encode_plus(text, padding='max_length', return_tensors="pt")
        input_ids, attention_masks = encoding["input_ids"].to(self.device), encoding["attention_mask"].to(self.device)

        outputs = self.model_t5.generate(
            input_ids=input_ids, attention_mask=attention_masks,
            max_length=self.config.max_length,
            do_sample=self.config.do_sample,
            top_k=self.config.top_k,
            top_p=self.config.top_p,
            repetition_penalty=self.config.repetition_penalty,
            early_stopping=self.config.early_stopping,
            num_return_sequences=self.config.num_return_sequences
        )

        gen_sentences_t5 = []
        for output in outputs:
            line = self.tokenizer_t5.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)
            gen_sentences_t5.append(line)
        
        return list(set(gen_sentences_t5))

    def generate_batch(self, sentences: List[str]) -> List[List[str]]:
        batch_outputs = []
        for sentence in sentences:
            batch_outputs.append(self.get_t5_gen_sentences(sentence))
        return batch_outputs


def main():
    # Load T5 model and tokenizer
    model_t5 = T5ForConditionalGeneration.from_pretrained('/home/qudratealahyratu/research/nlp/fact_checking/my_work/SciMedAttack/results/t5_paws_masked_claim_abstract_paws_3_epoch_2/models/model_3_epochs/')
    tokenizer_t5 = T5Tokenizer.from_pretrained('Vamsi/T5_Paraphrase_Paws')

    # Ensure model is in correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_t5 = model_t5.to(device)

    # Define model configuration
    config_params = {
            'max_length':512,
            'do_sample':True,
            'top_k':50,
            'top_p': 0.99,
            'repetition_penalty':3.5,
            'early_stopping':True,
            'num_return_sequences':10
    }
    config = ModelConfig(**config_params)

    # Generate paraphrases
    paraphraser_custom_t5 = T5Paraphraser(model_t5, tokenizer_t5, config)
    org_sentence = "This is a sentence to be paraphrased to be or not to be."
    paraphrases = paraphraser_custom_t5.get_t5_gen_sentences(org_sentence)
    # Print paraphrases
    for idx, paraphrase in enumerate(paraphrases, start=1):
        print(f"Paraphrase {idx}: {paraphrase}")

if __name__ == "__main__":
    main()
